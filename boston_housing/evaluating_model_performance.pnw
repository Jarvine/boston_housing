Evaluating Model Performance
============================
<<name='imports', echo=False>>=
# python standard library
import os
from distutils.util import strtobool

# third party
import numpy
from sklearn.datasets import load_boston
from sklearn.cross_validation import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.metrics import make_scorer
from sklearn.grid_search import GridSearchCV
from sklearn.tree import DecisionTreeRegressor
@
<<name='load_data', echo=False>>=
city_data = load_boston()
housing_prices = city_data.target
housing_features = city_data.data
DEBUG = strtobool(os.environ.get('DEBUG', 'off'))
@

<<name='train_test_split', echo=False>>=
def shuffle_split_data(X, y, test_size=.3, random_state=0):
    """ 
    Shuffles and splits data into training and testing subsets

    :param:
     - `X`: feature array
     - `y`: target array
     - `test_size`: fraction of data to use for testing
     - `random_state`: seed for the random number generator
    :return: x-train, y-train, x-test, y-test
    """
    X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size=test_size,
                                                         random_state=random_state)
    return X_train, y_train, X_test, y_test
@

.. currentmodule:: boston_housing.evaluating_model_performance

.. .. autosummary::
..    :toctree: api
.. 
..    shuffle_split_data
.. 
.. .. currentmodule:: sklearn.cross_validation
.. .. autosummary::
..    :toctree: api
.. 
..    train_test_split
   
Question 3
----------

*Why do we split the data into training and testing subsets for our model?*

We split the data into training and testing subsets so that we can assess the model using a different data-set than what it
was trained on, thus reducing the likelihood of overfitting the model to the training data and increasing the likelihood that it will generalize to other data.

<<name='train_test_split', echo=False>>=
def performance_metric(y_true, y_predict):
    """
    Calculates total error between true and predicted values

    :param:
     - `y_true`: array of target values
     - `y_predict`: array of values the model predicted
    :return: mean_squared_error for the prediction
    """
    return mean_squared_error(y_true, y_predict)


expected = 32.167
tolerance = 0.01
actual = performance_metric(numpy.arange(12), numpy.ones(12))
assert abs(expected - actual) < tolerance
@

.. .. currentmodule:: boston_housing.evaluating_model_performance
.. .. autosummary::
..    :toctree: api
.. 
..    performance_metric
.. 
.. .. currentmodule:: sklearn.metrics
.. .. autosummary::
..    :toctree: api
.. 
..    mean_squared_error
   
Question 4
----------

*Which performance metric below did you find was most appropriate for predicting housing prices and analyzing the total error. Why?* - *Accuracy* - *Precision* - *Recall* - *F1 Score* - *Mean Squared Error (MSE)* - *Mean Absolute Error (MAE)*

I chose *Mean Squared Error* as the most appropriate performance metric for predicting housing prices because we are predicting a numeric value (a regression problem) and while Mean Absolute Error could also be used, the MSE emphasizes larger errors more and so I felt it would be preferable.

Step 4 (Final Step)
-------------------

<<name='fit_model', echo=False>>=
def fit_model(X, y, k=10):
    """ 
    Tunes a decision tree regressor model using GridSearchCV

    :param:
     - `X`:  the input data
     - `y`:  target labels y
     - `k`: number of cross-validation folds
    :return: the optimal model
    """

    # Create a decision tree regressor object
    regressor = DecisionTreeRegressor()

    # Set up the parameters we wish to tune
    parameters = {'max_depth':(1,2,3,4,5,6,7,8,9,10)}

    # Make an appropriate scoring function
    scoring_function = make_scorer(mean_squared_error)

    # Make the GridSearchCV object
    reg = GridSearchCV(regressor, param_grid=parameters,
                       scoring=scoring_function, cv=k)

    # Fit the learner to the data to obtain the optimal model with tuned parameters
    reg.fit(X, y)

    # Return the optimal model
    return reg


# Test fit_model on entire dataset
reg = fit_model(housing_features, housing_prices)
if DEBUG:
    print( "Successfully fit a model!")
@

.. .. currentmodule:: boston_housing.evaluating_model_performance
.. .. autosummary::
..    :toctree: api
.. 
..    fit_model
.. 
.. .. currentmodule:: sklearn.tree
.. .. autosummary::
..    :toctree: api
.. 
..    DecisionTreeRegressor
   
Question 5
----------

*What is the grid search algorithm and when is it applicable?*

The `GridSearchCV <http://scikit-learn.org/stable/modules/grid_search.html>`_ algorithm exhaustively works through the parameters it is given to find the parameters that create the best model using cross-validation. Because it is exhaustive it is appropriate when the model-creation is not excessively computationally intensive, otherwise its run-time might be infeasible.

.. .. currentmodule:: sklearn.grid_search
.. 
.. .. autosummary::
..    :toctree: api
.. 
..    GridSearchCV

Question 6
----------


*What is cross-validation, and how is it performed on a model? Why would cross-validation be helpful when using grid search?*

Cross-validation is a method of testing a model by partitioning the data into subsets, with each subset taking a turn as the test set while the data not being used as a test-set is used as the training set. This allows the model to be tested against all the data-points, rather than having some data reserved exclusively as training data and the remainder exclusively as testing data.

