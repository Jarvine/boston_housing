Model Prediction
================
<<name='imports', echo=False>>=
# third-party
import matplotlib.pylab as plot
import numpy
import pandas
import seaborn
from sklearn.datasets import load_boston

# this code
from boston_housing.common import load_housing_data, CLIENT_FEATURES
from boston_housing.evaluating_model_performance import fit_model
from boston_housing.common import ValueCountsPrinter, print_image_directive
@
<<name='setup', echo=False>>=
housing_frame = pandas.read_hdf('data/housing_data.h5', 'table')
housing_data = load_housing_data()
housing_features = housing_data.features
housing_prices = housing_data.prices
seaborn.set_style('whitegrid')
seaborn.set_palette('husl')
@

Question 10
-----------

*Using grid search on the entire data set, what is the optimal  ``max_depth`` parameter for your model? How does this result compare to your initial intuition?*

To find the 'best' model I ran the `fit_model` function 1,000 times and took the `best_params_` (max-depth) and `best_score_` (negative MSE) for each trial.

<<name='find_optimal', echo=False>>=
# there appears to be a bug that will cause parallel jobs to break
# unless you set the environment variable JOBLIB_START_METHOD to 'forkserver'
# also may not work on some versions of python 2
# -1 means use parallel sub-processes
parallel_jobs = -1

# this will determine the running time overall for this code
repetitions = 1000
models = [fit_model(housing_features, housing_prices, n_jobs=parallel_jobs) for model in range(repetitions)]
params_scores = [(model.best_params_, model.best_score_) for model in models]
parameters = numpy.array([param_score[0]['max_depth'] for param_score in params_scores])
scores = numpy.array([param_score[1] for param_score in params_scores])
@
<<name='best_models_plot', echo=False, include=False, results='sphinx'>>=
best_models = pandas.DataFrame.from_dict({'parameter':parameters, 'score': scores})
x_labels = sorted(best_models.parameter.unique())
figure = plot.figure()
axe = figure.gca()
grid = seaborn.boxplot('parameter', 'score', data = best_models,
                       order=x_labels, ax=axe)
title = axe.set_title("Best Parameters vs Best Scores")
filename = 'best_parameters.png'
print_image_directive(filename, figure)
@

.. csv-table: Best Score
   :header: Description, Value
<<name='best_score', echo=False, results='sphinx'>>=
best_index = numpy.where(scores==numpy.max(scores))
print("   Best Score, {0:.2f}".format(scores[best_index][0]))
print("   max-depth parameter with best score,{0}".format(parameters[best_index][0]))
@

.. csv-table:: Parameter Counts
   :header: Max-Depth, Count
<<name='bin_counts', echo=False, results='sphinx'>>=
bin_range = best_models.parameter.max() - best_models.parameter.min()
bins = pandas.cut(best_models.parameter,
                  bin_range)
counts = bins.value_counts()
for bounds in counts.index:
    parameter = bounds.split(',')[0].lstrip('()')
    print('   {0},{1}'.format(int(round(float(parameter))),
                              counts.loc[bounds][0]))
@

.. csv-table:: Median Scores
   :header: Max-Depth, Median Score
<<name='best_median_scores', echo=False, results='sphinx'>>=
parameter_group = pandas.groupby(best_models, 'parameter')
medians = parameter_group.score.median()
for max_depth in medians.index:
    print('   {0},{1:.2f}'.format(max_depth, medians.loc[max_depth]))
@

.. csv-table:: Max Scores
   :header: Max-Depth, Max Score
<<name='best_scores', echo=False, results='sphinx'>>=
maxes = parameter_group.score.max()
for max_depth in maxes.index:
    print('   {0},{1:.2f}'.format(max_depth, maxes.loc[max_depth]))
@

.. note:: Since the `GridSearchCV` normally tries to maximize the output of the scoring-function, but the goal in this case was to minimize it, the scores are negations of the MSE, thus the higher the score, the lower the MSE.

While a max-depth of 4 was the most common best-parameter, the max-depth of 5 was the median max-depth, had the highest median score, and had the highest overall score, so I will say that the optimal `max_depth` parameter is 5. This is in line with what I had guessed, based on the Complexity Performance plot.

Question 11
-----------

*With your parameter-tuned model, what is the best selling price for your client's home? How does this selling price compare to the basic statistics you calculated on the dataset?*

.. '

.. csv-table:: Predicted Price
   :delim: ;
<<name='predicted_price', echo=False, results='sphinx'>>=
best_model = models[best_index[0][0]]
sale_price = best_model.predict(CLIENT_FEATURES)
predicted = sale_price[0] * 1000
actual_median = housing_frame.median_value.median() * 1000
print("   Predicted value of client's home; ${0:,.2f}".format(predicted))
print("   Median for all suburbs - predicted; ${0:,.2f}".format(actual_median - predicted))
@

My three chosen features (`lower_status`, `nitric_oxide`, and `rooms`) seemed to indicate that the client's house might be a lower-valued house, and the predicted value was about $232 less than the median median-value, so our model predicts that the client has a below-median-value house.

.. '

Question 12
-----------

*In a few sentences, discuss whether you would use this model or not to predict the selling price of future clients' homes in the Greater Boston area.*

.. '

I think that this model seems reasonable for the given data (Boston Suburbs in 1970), but I think that I might be hesitant to predict the value for a specific house using it, given that we are using aggregate-values for entire suburbs, not values for individual houses. I would also think that separating out the upper-class houses would give a better model for certain clients, given the right-skew of the data. Also, the median MSE for the best model was ~32 so taking the square root of this gives an 'average' error of about $5,700, which seems fairly high, given the low median-values for the houses. I think that the model gives a useful ball-park-figure estimate, but I think I'd have to qualify the certainty of prediction for future clients, noting also the age of the data and not extrapolating much beyond 1970.
