Model Prediction
================
<<name='imports', echo=False>>=
# third-party
import matplotlib.pylab as plot
import numpy
import pandas
import seaborn
from sklearn.datasets import load_boston

# this code
from boston_housing.common import load_housing_data, CLIENT_FEATURES
from boston_housing.evaluating_model_performance import fit_model
from boston_housing.common import ValueCountsPrinter
@
<<name='setup', echo=False>>=
housing_frame = pandas.read_hdf('data/housing_data.h5', 'table')
housing_data = load_housing_data()
housing_features = housing_data.features
housing_prices = housing_data.prices
seaborn.set_style('whitegrid')
@

Question 10
-----------

*Using grid search on the entire data set, what is the optimal  ``max_depth`` parameter for your model? How does this result compare to your initial intuition?*

<<name='find_optimal', echo=False>>=
# there appears to be a bug that will cause parallel jobs to break
# unless you run python 3.4 or greater
# and set the environment variable JOBLIB_START_METHOD to 'forkserver'
# so if you aren't running python 3 don't try to run parallel jobs (set to 1)
parallel_jobs = -1

# this will determine the running time overall for this code
repetitions = 1000

models = [fit_model(housing_features, housing_prices, n_jobs=parallel_jobs) for model in range(repetitions)]
params_scores = [(model.best_params_, model.best_score_) for model in models]
parameters = numpy.array([param_score[0]['max_depth'] for param_score in params_scores])
scores = numpy.array([param_score[1] for param_score in params_scores])
@
<<name='best_models_plot', echo=False, include=False, results='sphinx'>>=
best_models = pandas.DataFrame.from_dict({'parameter':parameters, 'score': scores})
x_labels = sorted(best_models.parameter.unique())
figure = plot.figure()
axe = figure.gca()
grid = seaborn.boxplot('parameter', 'score', data = best_models,
                       order=x_labels, ax=axe)
grid = seaborn.swarmplot('parameter', 'score', data = best_models,
                         order=x_labels, ax=axe, color='w', alpha=0.5)
title = axe.set_title("Best Parameters vs Best Scores")
filename = 'figures/best_parameters.png'
figure.savefig(filename)
print('.. image:: {0}'.format(filename))
@

<<name='best_score', echo=False, results='sphinx'>>=
best_index = numpy.where(scores==numpy.max(scores))
print("Best Score: {0}:.2f".format(scores[best_index][0]))
print("max-depth parameter with best score: {0}".format(parameters[best_index][0]))
@

.. csv-table:: Parameter Counts
   :header: Max-Depth, Count
<<name='bin_counts', echo=False, results='sphinx'>>=
bin_range = best_models.parameter.max() - best_models.parameter.min()
bins = pandas.cut(best_models.parameter,
                  bin_range)
counts = bins.value_counts()
for bounds in counts.index:
    parameter = bounds.split(',')[0].lstrip('()')
    print('   {0},{1}'.format(parameter, counts.loc[bounds][0]))
@

.. csv-table:: Median Scores
   :header: Max-Depth, Median Score
<<name='best_median_scores', echo=False, results='sphinx'>>=
parameter_group = pandas.groupby(best_models, 'parameter')
medians = parameter_group.score.median()
for max_depth in medians.index:
    print('   {0},{1:.2f}'.format(max_depth, medians.loc[max_depth]))
@

.. csv-table:: Max Scores
   :header: Max-Depth, Max Score
<<name='best_scores', echo=False, results='sphinx'>>=
maxes = parameter_group.score.max()
for max_depth in medians.index:
    print('   {0},{1:.2f}'.format(max_depth, medians.loc[max_depth]))
@

While a max-depth of 4 was the most common best-parameter, the max-depth of 5 was the median max-depth, had the highest median score, and had the highest overall score, so I will say that the optimal `max_depth` parameter is 5. This is slightly lower than my guess of 6, but doesn't seem too far off, although a max-depth of 7 seems to be a slight improvement over 6 as well.

.. '

Question 11
-----------

*With your parameter-tuned model, what is the best selling price for your client's home? How does this selling price compare to the basic statistics you calculated on the dataset?*

.. '

<<name='predicted_price', echo=False>>=
best_model = models[best_index[0][0]]
sale_price = best_model.predict(CLIENT_FEATURES)
predicted = sale_price[0] * 1000
actual_median = housing_frame.median_value.median() * 1000
print ("Predicted value of client's home: ${0:,.2f}".format(predicted))
print("Median Value - predicted: ${0:,.2f}".format(actual_median - predicted))
@

My three chosen features (`lower_status`, `nitric_oxide`, and `rooms`) seemed to indicate that the client's house might be a lower-valued house, and the predicted value was about $1,266 less than the median median-value, so our model predicts that the client has a below-median-value house.

.. '

Question 12
-----------

*In a few sentences, discuss whether you would use this model or not to predict the selling price of future clients' homes in the Greater Boston area.*

.. '

I think that this model seems to make a reasonable prediction for the given data (Boston Suburbs in 1970), but I'm not sure that I agree that the data for an entire suburb is necessarily generalizable for a specific unit within a suburb. What this model predicts is that a suburb with attributes similar to the client's would have our predicted median value, but within each suburb there would likely be a bit of variance. I would also think that separating out the upper-class houses would give a better model for this particular client, given the sub-median values for his or her features, and the right-skew of the data. If the goal was to predict median prices for suburbs, then I would use this model, but not for individual sales.



